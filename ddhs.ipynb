{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **인코더 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, num_classes):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, latent_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Classification Layer\n",
    "        self.classifier = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        class_pred = self.classifier(z)\n",
    "        return x_recon, z, class_pred\n",
    "\n",
    "\n",
    "\n",
    "def target_class_center_loss(z, labels, target_label=1):\n",
    "\n",
    "    \n",
    "    mask = labels == 0\n",
    "    z_false = z[mask]\n",
    "    # 선택된 데이터의 평균 계산\n",
    "    z_false_mean = z_false.mean(dim=0)\n",
    "\n",
    "    mask = labels == 1\n",
    "    z_true = z[mask]\n",
    "    # 선택된 데이터의 평균 계산\n",
    "    z_true_mean = z_true.mean(dim=0)\n",
    "   \n",
    "    loss_false = ((z_false - z_false_mean) ** 2).sum()/2\n",
    "    loss_true = ((z_true - z_true_mean) ** 2).sum()/2\n",
    "    loss = loss_true + loss_false\n",
    "    return loss\n",
    "def mape_loss(output, target, eps=1e-2):\n",
    "\n",
    "    return 100 * torch.mean(torch.abs((target - output) / (target + eps)))\n",
    "def train_model(model, dataloader, num_epochs):\n",
    "    criterion_recon = nn.MSELoss()\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, z, class_pred = model(data)\n",
    "\n",
    "            \n",
    "            loss_recon = criterion_recon(x_recon, data)*1000\n",
    "            loss_ce = criterion_ce(class_pred, labels)\n",
    "            \n",
    "            # Calculate the target class center loss for label 1\n",
    "            loss_target_class_center = target_class_center_loss(z, labels, target_label=1)\n",
    "      \n",
    "            # Combine the losses\n",
    "            loss = loss_recon + loss_ce + loss_target_class_center\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Setup your dataloader here as shown before\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        # Assuming 'is_converted' is the label column\n",
    "        self.data = torch.tensor(dataframe.drop(columns=['is_converted']).values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(dataframe['is_converted'].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 618.5135\n",
      "Epoch [2/100], Loss: 446.0736\n",
      "Epoch [3/100], Loss: 341.5865\n",
      "Epoch [4/100], Loss: 358.9991\n",
      "Epoch [5/100], Loss: 265.0999\n",
      "Epoch [6/100], Loss: 294.7734\n",
      "Epoch [7/100], Loss: 253.7092\n",
      "Epoch [8/100], Loss: 237.8191\n",
      "Epoch [9/100], Loss: 259.3097\n",
      "Epoch [10/100], Loss: 249.6816\n",
      "Epoch [11/100], Loss: 288.4279\n",
      "Epoch [12/100], Loss: 232.7352\n",
      "Epoch [13/100], Loss: 236.3727\n",
      "Epoch [14/100], Loss: 242.5010\n",
      "Epoch [15/100], Loss: 220.8013\n",
      "Epoch [16/100], Loss: 225.6420\n",
      "Epoch [17/100], Loss: 217.5265\n",
      "Epoch [18/100], Loss: 228.5418\n",
      "Epoch [19/100], Loss: 236.7931\n",
      "Epoch [20/100], Loss: 228.0318\n",
      "Epoch [21/100], Loss: 211.6195\n",
      "Epoch [22/100], Loss: 207.7432\n",
      "Epoch [23/100], Loss: 210.2309\n",
      "Epoch [24/100], Loss: 206.4343\n",
      "Epoch [25/100], Loss: 203.6395\n",
      "Epoch [26/100], Loss: 212.6441\n",
      "Epoch [27/100], Loss: 199.8744\n",
      "Epoch [28/100], Loss: 200.7134\n",
      "Epoch [29/100], Loss: 209.4112\n",
      "Epoch [30/100], Loss: 193.5911\n",
      "Epoch [31/100], Loss: 193.4958\n",
      "Epoch [32/100], Loss: 183.3506\n",
      "Epoch [33/100], Loss: 194.5732\n",
      "Epoch [34/100], Loss: 184.1227\n",
      "Epoch [35/100], Loss: 196.4152\n",
      "Epoch [36/100], Loss: 188.8394\n",
      "Epoch [37/100], Loss: 180.3517\n",
      "Epoch [38/100], Loss: 182.7166\n",
      "Epoch [39/100], Loss: 174.6624\n",
      "Epoch [40/100], Loss: 193.8704\n",
      "Epoch [41/100], Loss: 199.0132\n",
      "Epoch [42/100], Loss: 185.1786\n",
      "Epoch [43/100], Loss: 179.1464\n",
      "Epoch [44/100], Loss: 182.9365\n",
      "Epoch [45/100], Loss: 176.0875\n",
      "Epoch [46/100], Loss: 181.0482\n",
      "Epoch [47/100], Loss: 195.2859\n",
      "Epoch [48/100], Loss: 166.6989\n",
      "Epoch [49/100], Loss: 168.7408\n",
      "Epoch [50/100], Loss: 163.8849\n",
      "Epoch [51/100], Loss: 164.0054\n",
      "Epoch [52/100], Loss: 162.1331\n",
      "Epoch [53/100], Loss: 162.0893\n",
      "Epoch [54/100], Loss: 169.7383\n",
      "Epoch [55/100], Loss: 155.9799\n",
      "Epoch [56/100], Loss: 154.9201\n",
      "Epoch [57/100], Loss: 162.8926\n",
      "Epoch [58/100], Loss: 158.0856\n",
      "Epoch [59/100], Loss: 151.3210\n",
      "Epoch [60/100], Loss: 152.0244\n",
      "Epoch [61/100], Loss: 148.1122\n",
      "Epoch [62/100], Loss: 152.6714\n",
      "Epoch [63/100], Loss: 160.8779\n",
      "Epoch [64/100], Loss: 146.0540\n",
      "Epoch [65/100], Loss: 143.5270\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = train.shape[1] - 1  # Adjust based on your dataset\n",
    "latent_dim = 32\n",
    "num_classes = 2  # Adjust based on your dataset\n",
    "\n",
    "# Initialize the model\n",
    "model = Autoencoder(input_dim, latent_dim, num_classes)\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = CustomDataset(imbalanced_train)  # Make sure to replace 'train' with 'df'\n",
    "dataloader = DataLoader(dataset, batch_size=5000, shuffle=True)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "trained_encoder = train_model(model, dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "def extract_middle_percent(data, start, last):\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(data_scaled)\n",
    "    \n",
    "    log_prob = kde.score_samples(data_scaled)\n",
    "    prob = np.exp(log_prob)\n",
    "    threshold_low, threshold_high = np.percentile(prob, [start, last])\n",
    "    mask = np.logical_and(prob >= threshold_low, prob <= threshold_high) #######\n",
    "    data_keep = data[mask]\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # # Original KDE plot\n",
    "    # sns.kdeplot(data.ravel(), shade=True, label='Original KDE')\n",
    "    if len(data_keep) > 0 :\n",
    "      return data_keep,  data[~mask]\n",
    "    else:  \n",
    "      print(\"No middle 50% found, returning original data\")\n",
    "      return np.array([])\n",
    "\n",
    "  #  각 feature 안의 값을 복원추출하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<ReluBackward0>),\n",
       " tensor([[0.2559, 0.0641, 0.0152,  ..., 0.0094, 0.0132, 0.0120],\n",
       "         [0.2468, 0.0520, 0.0104,  ..., 0.0063, 0.0089, 0.0081],\n",
       "         [0.2419, 0.0473, 0.0087,  ..., 0.0052, 0.0075, 0.0068],\n",
       "         ...,\n",
       "         [0.2454, 0.0499, 0.0097,  ..., 0.0058, 0.0083, 0.0075],\n",
       "         [0.2484, 0.0546, 0.0113,  ..., 0.0069, 0.0097, 0.0088],\n",
       "         [0.2401, 0.0438, 0.0076,  ..., 0.0045, 0.0065, 0.0059]],\n",
       "        grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_true = train[train['is_converted']==1]\n",
    "train_false = train[train['is_converted']==0]\n",
    "dataset = CustomDataset(train_true)  # Make sure to replace 'train' with 'df'\n",
    "dataloader = DataLoader(dataset, batch_size=train_true.shape[0], shuffle=True)\n",
    "for data, label in dataloader:\n",
    "    x_recon, z, class_pred = trained_encoder(data)\n",
    "\n",
    "z,x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true = train[train['is_converted']==1]\n",
    "train_false = train[train['is_converted']==0]\n",
    "\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = CustomDataset(train_true)  # Make sure to replace 'train' with 'df'\n",
    "dataloader = DataLoader(dataset, batch_size=train_true.shape[0], shuffle=True)\n",
    "for data, label in dataloader:\n",
    "    x_recon, z, class_pred = trained_encoder(data)\n",
    "data_keep, data_sample = extract_middle_percent(z.detach().numpy(),25,100)  \n",
    "\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = CustomDataset(train_false)  # Make sure to replace 'train' with 'df'\n",
    "dataloader = DataLoader(dataset, batch_size=train_false.shape[0], shuffle=True)\n",
    "for data, label in dataloader:\n",
    "    x_recon, z, class_pred = trained_encoder(data)\n",
    "\n",
    "data_keep, data_drop = extract_middle_percent(z.detach().numpy(),50,100)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 생성 전 기준개수: 27225\n",
      "데이터 생성 전 개수: 1213\n",
      "100번 중 유효한 행의 수: 26012\n",
      "데이터 생성 후 개수: 27225\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# data_sample과 data_keep을 텐서로 변환 (이미 텐서라면 이 단계는 생략 가능)\n",
    "data_sample = torch.tensor(data_sample)\n",
    "data_keep = torch.tensor(data_keep)\n",
    "\n",
    "# Step 1: data_sample과 data_keep의 평균 구하기\n",
    "data_true_mean = data_sample.mean(dim=0)\n",
    "data_false_mean = data_keep.mean(dim=0)\n",
    "\n",
    "valid_count = 0  # 유효한 행의 개수를 세기 위한 변수\n",
    "print(f\"데이터 생성 전 기준개수: {data_keep.shape[0]}\")\n",
    "print(f\"데이터 생성 전 개수: {data_sample.shape[0]}\")\n",
    "while(data_sample.shape[0]<data_keep.shape[0]):\n",
    "    # 각 열별로 랜덤 인덱스 선택\n",
    "    random_indices = torch.randint(0, data_sample.shape[0], size=(1, data_sample.shape[1]))\n",
    "    new_row = torch.gather(data_sample, 0, random_indices)\n",
    "\n",
    "    # Step 2: 가장 외곽에 있는 행과의 임계거리 계산\n",
    "    distances = torch.norm(data_sample - data_true_mean, dim=1)\n",
    "    threshold_distance = distances.max()\n",
    "\n",
    "    # Step 3: 새로운 행과 data_sample 평균과의 거리 계산\n",
    "    distance_true = torch.norm(new_row - data_true_mean)\n",
    "    distance_false = torch.norm(new_row - data_false_mean)\n",
    "\n",
    "    # 새로운 행의 유효성 판단\n",
    "    if distance_true <= threshold_distance and distance_true <= distance_false:\n",
    "        valid_count += 1\n",
    "        # 조건을 충족하는 경우 new_row를 data_sample에 추가\n",
    "        data_sample = torch.cat((data_sample, new_row), dim=0)\n",
    "    \n",
    "print(f\"100번 중 유효한 행의 수: {valid_count}\")\n",
    "print(f\"데이터 생성 후 개수: {data_sample.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoded_data = trained_encoder.decoder(data_sample)\n",
    "\n",
    "# 디코딩된 데이터를 numpy 배열로 변환 (필요한 경우)\n",
    "decoded_data_np = decoded_data.detach().numpy()  # GPU에서 실행하는 경우 .cpu()를 추가해야 할 수 있음\n",
    "\n",
    "column_names = X.columns\n",
    "\n",
    "# 디코딩된 데이터를 데이터프레임으로 변환하고 열 이름 적용\n",
    "true_df = pd.DataFrame(decoded_data_np, columns=column_names)\n",
    "true_df['is_converted']=1\n",
    "\n",
    "decoded_data = trained_encoder.decoder(data_keep)\n",
    "\n",
    "# 디코딩된 데이터를 numpy 배열로 변환 (필요한 경우)\n",
    "decoded_data_np = decoded_data.detach().numpy()  # GPU에서 실행하는 경우 .cpu()를 추가해야 할 수 있음\n",
    "\n",
    "column_names = X.columns\n",
    "\n",
    "# 디코딩된 데이터를 데이터프레임으로 변환하고 열 이름 적용\n",
    "false_df = pd.DataFrame(decoded_data_np, columns=column_names)\n",
    "false_df['is_converted']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train = pd.concat([true_df, false_df], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = balanced_train[balanced_train.columns.drop('is_converted')]\n",
    "Y = balanced_train['is_converted']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05, random_state=42, shuffle=True)\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = test.drop([\"is_converted\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_converted\n",
       "0    1370\n",
       "1    1353\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "def get_clf_eval(y_test, y_pred=None):\n",
    "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
    "\n",
    "    print(\"오차행렬:\\n\", confusion)\n",
    "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
    "    print(\"정밀도: {:.4f}\".format(precision))\n",
    "    print(\"재현율: {:.4f}\".format(recall))\n",
    "    print(\"F1: {:.4f}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "# XGBoost와 CatBoost 라이브러리를 임포트합니다.\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "def objective(trial):\n",
    "    # classifier_name = trial.suggest_categorical('classifier', ['DecisionTree', 'LGBM', 'XGBoost', 'CatBoost'])\n",
    "    classifier_name = 'XGBoost'\n",
    "    if classifier_name == 'DecisionTree':\n",
    "        param = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'max_features': trial.suggest_int('max_features', 1, 30)\n",
    "        }\n",
    "        model = DecisionTreeClassifier(**param)\n",
    "    elif classifier_name == 'LGBM':\n",
    "        param = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 300, 500),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "\n",
    "        }\n",
    "        model = LGBMClassifier(**param)\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        param = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "            'random_state':trial.suggest_int('random_state',42,42),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "    elif classifier_name == 'CatBoost':\n",
    "        param = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10),\n",
    "            'border_count': trial.suggest_int('border_count', 1, 255),\n",
    "            'loss_function': 'Logloss',\n",
    "        }\n",
    "        model = cb.CatBoostClassifier(**param, verbose=False)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_pred = model.predict(x_test)\n",
    "    print(sum(test_pred))\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = f1_score(y_test, y_pred, average='binary')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-24 20:48:26,606] A new study created in memory with name: no-name-336a2cc2-5332-42df-915a-52cf16d8bd6b\n",
      "C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "[I 2024-02-24 20:48:30,803] Trial 0 finished with value: 0.997779422649889 and parameters: {'n_estimators': 250, 'learning_rate': 0.2536999076681772, 'max_depth': 8, 'min_child_weight': 6, 'subsample': 0.5780093202212182, 'random_state': 42, 'colsample_bytree': 0.5779972601681014}. Best is trial 0 with value: 0.997779422649889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "[I 2024-02-24 20:48:32,820] Trial 1 finished with value: 0.997039230199852 and parameters: {'n_estimators': 123, 'learning_rate': 0.19030368381735815, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.5102922471479012, 'random_state': 42, 'colsample_bytree': 0.9849549260809971}. Best is trial 0 with value: 0.997779422649889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "[I 2024-02-24 20:48:39,210] Trial 2 finished with value: 0.9970348406226834 and parameters: {'n_estimators': 433, 'learning_rate': 0.020589728197687916, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.6521211214797689, 'random_state': 42, 'colsample_bytree': 0.762378215816119}. Best is trial 0 with value: 0.997779422649889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "[I 2024-02-24 20:48:43,641] Trial 3 finished with value: 0.9974064468321601 and parameters: {'n_estimators': 273, 'learning_rate': 0.02692655251486473, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.6460723242676091, 'random_state': 42, 'colsample_bytree': 0.6831809216468459}. Best is trial 0 with value: 0.997779422649889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "[I 2024-02-24 20:48:46,893] Trial 4 finished with value: 0.997039230199852 and parameters: {'n_estimators': 282, 'learning_rate': 0.14447746112718687, 'max_depth': 4, 'min_child_weight': 6, 'subsample': 0.7962072844310213, 'random_state': 42, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 0.997779422649889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "[I 2024-02-24 20:48:50,310] Trial 5 finished with value: 0.9962908011869436 and parameters: {'n_estimators': 343, 'learning_rate': 0.0178601378893971, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.9828160165372797, 'random_state': 42, 'colsample_bytree': 0.9041986740582306}. Best is trial 0 with value: 0.997779422649889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "[I 2024-02-24 20:48:53,743] Trial 6 finished with value: 0.9970348406226834 and parameters: {'n_estimators': 222, 'learning_rate': 0.013940346079873234, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.5610191174223894, 'random_state': 42, 'colsample_bytree': 0.7475884550556351}. Best is trial 0 with value: 0.997779422649889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "[W 2024-02-24 20:48:54,055] Trial 7 failed with parameters: {'n_estimators': 113, 'learning_rate': 0.22038218939289875, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.6558555380447055, 'random_state': 42, 'colsample_bytree': 0.7600340105889054} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\kuils\\AppData\\Local\\Temp\\ipykernel_31324\\2742280551.py\", line 52, in objective\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"c:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1519, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"c:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\core.py\", line 2051, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-02-24 20:48:54,057] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[168], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m sampler \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39msampler)\n\u001b[1;32m----> 4\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39m# 최적화 결과 출력\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of finished trials:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(study\u001b[39m.\u001b[39mtrials))\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[167], line 52\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     41\u001b[0m     param \u001b[39m=\u001b[39m {\n\u001b[0;32m     42\u001b[0m         \u001b[39m'\u001b[39m\u001b[39miterations\u001b[39m\u001b[39m'\u001b[39m: trial\u001b[39m.\u001b[39msuggest_int(\u001b[39m'\u001b[39m\u001b[39miterations\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m500\u001b[39m),\n\u001b[0;32m     43\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: trial\u001b[39m.\u001b[39msuggest_loguniform(\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.3\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mLogloss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     48\u001b[0m     }\n\u001b[0;32m     49\u001b[0m     model \u001b[39m=\u001b[39m cb\u001b[39m.\u001b[39mCatBoostClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparam, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 52\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     54\u001b[0m test_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_test)\n\u001b[0;32m     55\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39m(test_pred))\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\sklearn.py:1519\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1491\u001b[0m (\n\u001b[0;32m   1492\u001b[0m     model,\n\u001b[0;32m   1493\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1499\u001b[0m )\n\u001b[0;32m   1500\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1501\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1502\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1517\u001b[0m )\n\u001b[1;32m-> 1519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1520\u001b[0m     params,\n\u001b[0;32m   1521\u001b[0m     train_dmatrix,\n\u001b[0;32m   1522\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1523\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1524\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1525\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1526\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1527\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1528\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1529\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1530\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1531\u001b[0m )\n\u001b[0;32m   1533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kuils\\anaconda3\\envs\\env\\lib\\site-packages\\xgboost\\core.py:2051\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2047\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2049\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2050\u001b[0m     _check_call(\n\u001b[1;32m-> 2051\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\n\u001b[0;32m   2052\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mc_int(iteration), dtrain\u001b[39m.\u001b[39;49mhandle\n\u001b[0;32m   2053\u001b[0m         )\n\u001b[0;32m   2054\u001b[0m     )\n\u001b[0;32m   2055\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2056\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optuna Study 생성 및 최적화\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 최적화 결과 출력\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
